{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import cvxpy as cp\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_price(symbol, comparison_symbol):\n",
    "    \"\"\"\n",
    "        Gets the single price of decentralized value and converts to any centralized value\n",
    "        fysm: Blockchain Type\n",
    "        tsyms: Centralized Coin\n",
    "    \"\"\"\n",
    "    api_key = \"insert-your-api-key\"\n",
    "    url = \"https://min-api.cryptocompare.com/data/price\"\n",
    "    payload = {\"fsym\": symbol.upper(), \"tsyms\": comparison_symbol.upper()}\n",
    "    headers = {\"authorization\": \"Apikey \" + api_key}\n",
    "    result = requests.get(url, headers = headers, params = payload).json()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_prices(symbol, comparison_symbol, limit, aggregate = 1):\n",
    "    \"\"\"\n",
    "        Gets the prices of decentralized value occured with an hour break and converts to any centralized value\n",
    "        fysm: Blockchain Type\n",
    "        tsyms: Centralized Coin\n",
    "        limit: Number of Data\n",
    "        aggregate: Grouping the Same Data\n",
    "    \"\"\"\n",
    "    api_key = \"insert-your-api-key\"\n",
    "    url = \"https://min-api.cryptocompare.com/data/histohour\"\n",
    "    payload = {\"fsym\": symbol.upper(), \"tsym\": comparison_symbol.upper(), \"limit\": limit, \"aggregate\": aggregate}\n",
    "    headers = {\"authorization\": \"Apikey \" + api_key}\n",
    "    result = requests.get(url, headers = headers, params = payload).json()\n",
    "    df = pd.DataFrame(result['Data'])\n",
    "    if 'time' in df:\n",
    "        df['timestamp'] = [datetime.datetime.fromtimestamp(d) for d in df.time]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minute_prices(symbol, comparison_symbol, limit, aggregate = 1):\n",
    "    \"\"\"\n",
    "        Gets the prices of decentralized value occured with a minute break and converts to any centralized value\n",
    "        fysm: Blockchain Type\n",
    "        tsyms: Centralized Coin\n",
    "        limit: Number of Data\n",
    "        aggregate: Grouping the Same Data\n",
    "    \"\"\"    \n",
    "    api_key = \"insert-your-api-key\"\n",
    "    url = \"https://min-api.cryptocompare.com/data/histominute\"\n",
    "    payload = {\"fsym\": symbol.upper(), \"tsym\": comparison_symbol.upper(), \"limit\": limit, \"aggregate\": aggregate}\n",
    "    headers = {\"authorization\": \"Apikey \" + api_key}\n",
    "    result = requests.get(url, headers = headers, params = payload).json()\n",
    "    df = pd.DataFrame(result['Data'])\n",
    "    if 'time' in df:\n",
    "        df['timestamp'] = [datetime.datetime.fromtimestamp(d) for d in df.time]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(df_coin, time_coin, symbol, comparison_symbol):\n",
    "    \"\"\"\n",
    "        Draw the graph of the distribution of coin data\n",
    "    \"\"\" \n",
    "    plt.plot(df_coin.timestamp, df_coin.close)\n",
    "    plt.title('OHLCV: ' + time_coin, fontsize = 24)\n",
    "    plt.ylabel('Price: ' + symbol.upper() + ' to ' + comparison_symbol.upper(), fontsize = 14)\n",
    "    plt.xlabel('Timestamp as '+ time_coin, fontsize = 14)\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_hyperparameter(k_hyperparameter, error_rate, classification = False):\n",
    "    \"\"\"\n",
    "        Draw the graph of the distribution of hyperparamters and error rates\n",
    "    \"\"\" \n",
    "    plt.plot(k_hyperparameter, error_rate)\n",
    "    if classification == False:\n",
    "        plt.title('Hyperparameters and Error Rates', fontsize = 24)\n",
    "    else:\n",
    "        plt.title('Hyperparameters and Maximum Accuracy', fontsize = 24)\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.xlabel('Hyperparameter')\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hoeffding(error_rate, confidence_rate):\n",
    "    \"\"\"\n",
    "        To calculate the minimum number of data needed\n",
    "    \"\"\"\n",
    "    result = math.log(2 / (1 - confidence_rate), 2) / (2 * error_rate ** 2)\n",
    "    return int(result + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "        This class is created to implement the linear regression from scratch.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_x, train_y, valid_x, valid_y, alpha = 0, regularization_type = \"\"):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.valid_x = valid_x\n",
    "        self.valid_y = valid_y\n",
    "        self.alpha = alpha # Tuning parameter\n",
    "        self.regularization_type = regularization_type\n",
    "    def least_squares(self, train_x, train_y):\n",
    "        \"\"\"\n",
    "            Use Sum of Least Squares without any Regularization Technique\n",
    "            Find the beta_ls using the vectorized version of predictor and the response variable \n",
    "            and return the parameter vector.\n",
    "        \"\"\"\n",
    "        beta_least_squares = np.zeros((1, 5))\n",
    "        # Vanilla regression to check if it needs a regularization\n",
    "        if np.linalg.det(np.matmul(train_x.transpose(), train_x)) != 0:\n",
    "            beta_least_squares = np.matmul(np.matmul(np.linalg.inv(np.matmul(train_x.transpose(), train_x)), train_x.transpose()),train_y)\n",
    "        return beta_least_squares\n",
    "    def ridge_regression(self, train_x, train_y, alpha):\n",
    "        \"\"\"\n",
    "            Standardize the predictors and center the response\n",
    "            Find beta_ridge as the formula suggests and return it\n",
    "        \"\"\"\n",
    "        mean_x = np.mean(train_x)\n",
    "        mean_y = np.mean(train_y)\n",
    "        std_x = np.std(train_x)\n",
    "        x_standard = (train_x - mean_x) / std_x\n",
    "        y_centered = train_y - mean_y\n",
    "        x_standard_transpose = x_standard.transpose()\n",
    "        beta_ridge_regression = np.matmul(np.matmul((np.linalg.inv(np.matmul(x_standard_transpose, x_standard) + alpha)), x_standard_transpose), y_centered)\n",
    "        return beta_ridge_regression\n",
    "    def lasso(self, train_x, train_y, alpha):\n",
    "        \"\"\"\n",
    "            Standardize the predictors and center the response\n",
    "            Find beta_lasso using quadratic programming package and return it.\n",
    "        \"\"\"\n",
    "        mean_x = np.mean(train_x)\n",
    "        mean_y = np.mean(train_y)\n",
    "        std_x = np.std(train_x)\n",
    "        x_standard = (train_x - mean_x) / std_x\n",
    "        y_centered = train_y - mean_y\n",
    "        x_standard_transpose = x_standard.transpose()\n",
    "        y_centered_transpose = y_centered.transpose()\n",
    "        gamma = cp.Parameter(nonneg=True)\n",
    "        # Construct the problem.\n",
    "        beta_lasso = cp.Variable((train_x.shape[1], 1))\n",
    "        xbeta = (x_standard *  beta_lasso)\n",
    "        error = cp.sum_squares(xbeta - np.array(y_centered).reshape(1800, 1))\n",
    "        obj = cp.Minimize(error + gamma * cp.norm(beta_lasso, 1))\n",
    "        prob = cp.Problem(obj)\n",
    "        gamma.value = alpha\n",
    "        prob.solve()\n",
    "        return beta_lasso.value\n",
    "    def fit_ls(self, train_x, train_y):\n",
    "        \"\"\"\n",
    "            Fits to the training set using Ordinary Least Squares Parameter Vector and returns the error.\n",
    "        \"\"\"\n",
    "        if self.alpha != 0:\n",
    "            pass\n",
    "        else:\n",
    "            prediction = self.predict(train_x, train_y, 0, \"\")\n",
    "            error_rmse = math.sqrt(prediction[1] / train_x.shape[0])\n",
    "        return error_rmse \n",
    "    def cross_validation(self, valid_x, valid_y, alpha = 0, regularization_type = \"\"):\n",
    "        \"\"\"\n",
    "            Find the best hyperparameter using 10-fold cross validation method.\n",
    "            Return the tuning parameter to use in the unseen data and \n",
    "            the error to make the analysis of the performance of model.\n",
    "        \"\"\"\n",
    "        if alpha == 0: # Use vanilla regression\n",
    "            pass\n",
    "        else:\n",
    "            if regularization_type == \"ridge_regression\":\n",
    "                prediction = self.predict(valid_x, valid_y, alpha, \"ridge_regression\")\n",
    "            else:\n",
    "                prediction = self.predict(valid_x, valid_y, alpha, \"lasso\")\n",
    "        error_rmse = math.sqrt(prediction[1] / valid_x.shape[0])\n",
    "        return error_rmse   \n",
    "    def predict(self, test_x, test_y, alpha, regularization_type = \"\"):\n",
    "        \"\"\"\n",
    "            Predict ups and downs using OHLCV of Blockchains\n",
    "            Find the linear relationship between OHLCV features and result\n",
    "            Estimate the result\n",
    "            Check if it is increased or decreased, return it\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        error_rate = 0\n",
    "        # Standardize the predictors and center the response in the case of using a regularization\n",
    "        mean_x = np.mean(self.train_x)\n",
    "        mean_y = np.mean(self.train_y)\n",
    "        std_x = np.std(self.train_x)\n",
    "        test_x_standard = (test_x - mean_x) / std_x\n",
    "        y_centered = self.train_y - mean_y\n",
    "        if alpha == 0: # Use vanilla regression\n",
    "            beta = self.least_squares(self.train_x, self.train_y)\n",
    "            if np.count_nonzero(beta): # Check if it has lots of infinite solutions\n",
    "                y_predicted = np.matmul(test_x, beta)\n",
    "            else:\n",
    "                raise Exception(\"The parameter beta has infinite solutions !\")\n",
    "        else:\n",
    "            if regularization_type == \"ridge_regression\":\n",
    "                beta = self.ridge_regression(self.train_x, self.train_y, alpha)\n",
    "            else:\n",
    "                beta = self.lasso(self.train_x, self.train_y, alpha)\n",
    "            y_predicted = np.matmul(test_x_standard, beta) + mean_y # Find the estimator of y, recover the data\n",
    "        error_rate += np.sum((y_predicted - test_y) ** 2)\n",
    "        prediction = math.sqrt(error_rate / valid_x.shape[0])\n",
    "        return y_predicted, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN:\n",
    "    \"\"\"\n",
    "        This class is created to implement the slightly different version of kNN because of the higher dependency of datapoints.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_x, valid_x, train_y, valid_y, k_hyperparameter = 5, weighted = 0, distance_measure = 0):\n",
    "        self.train_x = train_x\n",
    "        self.k_hyperparameter = k_hyperparameter\n",
    "        self.valid_x = valid_x\n",
    "        self.weighted = weighted\n",
    "        self.distance_measure = distance_measure\n",
    "        self.train_y = train_y\n",
    "        self.valid_y = valid_y\n",
    "    def predict_kNN(self, train_x, k_hyperparameter, weighted = 0, distance_measure = 0):\n",
    "        \"\"\"\n",
    "            We have three distance measure and two weight techniques to calculate the optimal result in test data.\n",
    "            Details are given in the report.\n",
    "        \"\"\"\n",
    "        prob_ups = 0\n",
    "        prob_downs = 0\n",
    "        if distance_measure == 0:\n",
    "            \"\"\"\n",
    "                The model assumes all adjacent points are dependent.\n",
    "                It calculates up and down measurement in terms of adjacency and predicts according to the measurement.\n",
    "            \"\"\"\n",
    "            if not weighted:\n",
    "                \"\"\"\n",
    "                    Assumes the distance between points does not affect the result.\n",
    "                    Namely, all points are independent of each other.\n",
    "                \"\"\"\n",
    "                for i in range(self.train_x.shape[0] - k_hyperparameter - 2, self.train_x.shape[0] - 2):\n",
    "                    if self.train_x[i + 1, 2] - self.train_x[i, 2] > 0:\n",
    "                        prob_ups += 1\n",
    "                    else:\n",
    "                        prob_downs += 1\n",
    "            else:\n",
    "                \"\"\"\n",
    "                    Assumes that the weight has inversely proportional to the distance.\n",
    "                \"\"\"\n",
    "                for i in range(self.train_x.shape[0] - k_hyperparameter - 2, self.train_x.shape[0] - 2):\n",
    "                    result = i * math.log(k, 2)\n",
    "                    if self.train_x[i + 1, 2] - self.train_x[i, 2] > 0:\n",
    "                        prob_ups += result\n",
    "                    else:\n",
    "                        prob_downs += result\n",
    "        elif distance_measure == 1:\n",
    "            \"\"\"\n",
    "                The model assumes all points are dependent with the chosen point but independent from each other.\n",
    "                It calculates up and down measurement in terms of difference between chosen point and other points\n",
    "                and predicts according to the measurement.\n",
    "            \"\"\"\n",
    "            if not weighted:\n",
    "                for i in range(self.train_x.shape[0] - k_hyperparameter - 2, self.train_x.shape[0] - 2):\n",
    "                    if self.train_x[i + 1, 2] - self.train_x[i, 2] > 0:\n",
    "                        prob_ups += 1\n",
    "                    else:\n",
    "                        prob_downs += 1\n",
    "            else:\n",
    "                for i in range(self.train_x.shape[0] - k_hyperparameter - 2, self.train_x.shape[0] - 2):\n",
    "                    result = i * math.log(k, 2)\n",
    "                    if self.train_x[i + 1, 2] - self.train_x[i, 2] > 0:\n",
    "                        prob_ups += result\n",
    "                    else:\n",
    "                        prob_downs += result\n",
    "        else:\n",
    "            \"\"\"\n",
    "                The model assumes that all points with the same values are dependent of each other.\n",
    "                This comes from the famous phrase \"All that has happened before will happen again.\"\n",
    "            \"\"\"\n",
    "            measure_points = []\n",
    "            k_measure = 0\n",
    "            for i in range(self.train_x.shape[0] - 1, -1, -1):\n",
    "                if self.train_x[i, 2] == self.train_x[self.train_x.shape[0] - 1, 2]:\n",
    "                    measure_points.append(i)\n",
    "                    k_measure += 1\n",
    "                if k_measure >= k_hyperparameter:\n",
    "                    break\n",
    "            if not weighted:\n",
    "                for i in range(len(measure_points)):\n",
    "                    if self.train_x[measure_points[i] + 1, 2] - self.train_x[measure_points[i], 2] > 0:\n",
    "                        prob_ups += 1\n",
    "                    else:\n",
    "                        prob_downs += 1\n",
    "            else:\n",
    "                for i in range(len(measure_points)):\n",
    "                    result = i * math.log(k, 2)\n",
    "                    if self.train_x[i + 1, 2] - self.train_x[i, 2] > 0:\n",
    "                        prob_ups += result\n",
    "                    else:\n",
    "                        prob_downs += result\n",
    "        distribution = [prob_ups / (prob_ups + prob_downs), prob_downs / (prob_ups + prob_downs)]\n",
    "        return distribution\n",
    "    def cross_validation(self):\n",
    "        \"\"\"\n",
    "            Find the validation error using cross validation.\n",
    "            Classify the results and create a confusion matrix.\n",
    "            Calculate precision, recall and accuracy using the confusion matrix.\n",
    "        \"\"\"\n",
    "        predicted_actual = np.zeros((2, 2), dtype = int)\n",
    "        for i in range(self.valid_x.shape[0] - 2, -1, -1):\n",
    "            pred = self.predict_kNN(self.valid_x, self.k_hyperparameter, self.weighted, self.distance_measure)\n",
    "            rando = np.random.choice(np.arange(0, 2), p=pred) \n",
    "            if rando:\n",
    "                if self.valid_x[i + 1, 2] - self.valid_x[i, 2]:\n",
    "                    predicted_actual[1, 1] += 1\n",
    "                else:\n",
    "                    predicted_actual[1, 0] += 1\n",
    "            else:\n",
    "                if self.valid_x[i + 1, 2] - self.valid_x[i, 2]:\n",
    "                    predicted_actual[0, 1] += 1\n",
    "                else:\n",
    "                    predicted_actual[0, 0] += 1\n",
    "        precision = predicted_actual[1, 1] / (predicted_actual[1, 1] + predicted_actual[1, 0]) * 100\n",
    "        recall = predicted_actual[1, 1] / (predicted_actual[1, 1] + predicted_actual[0, 1]) * 100\n",
    "        accuracy = (predicted_actual[1, 1] + predicted_actual[0, 0]) / np.sum(predicted_actual) * 100\n",
    "        parameters = [precision, recall, accuracy]\n",
    "        return predicted_actual, parameters\n",
    "    def predict_test(self, test_x, k_hyperparameter):\n",
    "        \"\"\"\n",
    "            Predict the unseen data.\n",
    "            Classify the unseen data and compare with the actual data.\n",
    "        \"\"\"\n",
    "        predict_x = []\n",
    "        predict = 0\n",
    "        for i in range(self.train_x.shape[0]):\n",
    "            predict_x.append([(self.train_x[i, 2] - test_x[0, 2]), i])\n",
    "        predict_x.sort(key=lambda x:x[0])\n",
    "        least = 9999\n",
    "        index = 0\n",
    "        for i in range(len(predict_x)):\n",
    "            if abs(predict_x[i][0]) < least:\n",
    "                index = i\n",
    "                least = abs(predict_x[i][0])\n",
    "        ups = 0\n",
    "        downs = 0\n",
    "        low, high = 0, 0\n",
    "        if index + k_hyperparameter // 2 > self.train_x.shape[0]:\n",
    "            low = self.train_x.shape[0] - k_hyperparameter\n",
    "            high = self.train_x.shape[0]\n",
    "        elif index - k_hyperparameter // 2 < 0:\n",
    "            low = 0\n",
    "            high = k_hyperparameter\n",
    "        else:\n",
    "            low = index - k_hyperparameter // 2\n",
    "            high = index + k_hyperparameter // 2\n",
    "        for i in range(low, high):\n",
    "            if self.train_y[i] - self.train_x[i, 2] > 0:\n",
    "                ups += 1\n",
    "            else:\n",
    "                downs += 1\n",
    "        if ups > downs:\n",
    "            predict = 1\n",
    "        else:\n",
    "            predict = 0\n",
    "        return predict             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "        This class is created to implement the logistic regression from scratch.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_x, data_y, alpha = 0, regularization_type = \"LS\"):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.alpha = 0\n",
    "        self.learning_rate = 0\n",
    "        self.theta = None\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.valid_x = None\n",
    "        self.valid_y = None\n",
    "        self.regularization_type = regularization_type\n",
    "    def create_label(self, train_x, train_y):\n",
    "        \"\"\"\n",
    "            Creates the binary classification label of the training and the validation set.\n",
    "            Return the label set.\n",
    "        \"\"\"\n",
    "        class_y = train_y - train_x[:, 3]\n",
    "        class_y[class_y > 0] = 1\n",
    "        class_y[class_y <= 0] = 0\n",
    "        return class_y\n",
    "    def cost_function_derivative(self, train_x, theta):\n",
    "        \"\"\"\n",
    "            The implementation of sigmoid function as a cost function on the software.\n",
    "            Return the cost matrix.\n",
    "        \"\"\"\n",
    "        cost_matrix = np.zeros((train_x.shape[0], 1))\n",
    "        for i in range(0, train_x.shape[0]):\n",
    "            cost_matrix[i] = 1 / (1 + np.exp(- 1 * np.matmul(train_x[i], theta.transpose())))\n",
    "        return cost_matrix\n",
    "    def gradient_descent(self, train_x, train_y, alpha = 0, learning_rate = 0.1):\n",
    "        \"\"\"\n",
    "            Find the parameter vector using the gradient descent.\n",
    "            Return the cost and the parameter vector.\n",
    "        \"\"\"\n",
    "        theta = np.random.rand(1, train_x.shape[1])\n",
    "        theta = theta - np.mean(theta)\n",
    "        temp = np.zeros((1, train_x.shape[1]))\n",
    "        self.mean_x = np.mean(train_x)\n",
    "        self.std_x = np.std(train_x)\n",
    "        standard_x = (train_x - np.mean(train_x)) / np.std(train_x)\n",
    "        center_y = self.create_label(train_x, train_y)\n",
    "        cost = 0\n",
    "        if self.regularization_type == \"LS\" or self.regularization_type == \"RR\":  \n",
    "            temp[0, 0] = theta[0, 0] * (1 - learning_rate * alpha / train_x.shape[0]) - learning_rate * np.sum(self.cost_function_derivative(standard_x, theta) - center_y) / train_x.shape[0]\n",
    "            for i in range(1, train_x.shape[1]):\n",
    "                temp[0, i] = theta[0, i] * (1 - learning_rate * alpha / train_x.shape[0]) - learning_rate * np.sum(np.matmul((self.cost_function_derivative(standard_x, theta) - center_y).transpose(), standard_x)) / train_x.shape[0]\n",
    "            while True:\n",
    "                temp[0, 0] = theta[0, 0] * (1 - learning_rate * alpha / train_x.shape[0]) - learning_rate * np.sum(self.cost_function_derivative(standard_x, theta) - center_y)  / train_x.shape[0]\n",
    "                for i in range(1, train_x.shape[1]):\n",
    "                    temp[0, i] = theta[0, i] * (1 - learning_rate * alpha / train_x.shape[0]) - learning_rate * np.sum(np.matmul((self.cost_function_derivative(standard_x, theta) - center_y).transpose(), standard_x)) / train_x.shape[0]\n",
    "                if np.allclose(theta, temp, atol=1e-04) == True:\n",
    "                    break\n",
    "                else:\n",
    "                    theta = temp\n",
    "        elif self.regularization_type == \"LR\":\n",
    "            temp[0, 0] = theta[0, 0] * (1 - learning_rate * alpha / train_x.shape[0]) - learning_rate * np.sum(self.cost_function_derivative(standard_x, theta) - center_y) / train_x.shape[0]\n",
    "            for i in range(1, train_x.shape[1]):\n",
    "                temp[0, i] = theta[0, i] * (1 - learning_rate * alpha / train_x.shape[0]) - learning_rate * np.sum(np.matmul((self.cost_function_derivative(standard_x, theta) - center_y).transpose(), standard_x)) / train_x.shape[0]\n",
    "                if temp[0, i] > alpha / 2:\n",
    "                    temp[0, i] -= alpha / 2\n",
    "                elif temp[0, i] < alpha / 2:\n",
    "                    temp[0, i] += alpha / 2\n",
    "                else:\n",
    "                    temp[0, i] = 0\n",
    "            while True:\n",
    "                temp[0, 0] = theta[0, 0] * (1 - learning_rate * alpha / train_x.shape[0]) - learning_rate * np.sum(self.cost_function_derivative(standard_x, theta) - center_y) / train_x.shape[0]\n",
    "                for i in range(1, train_x.shape[1]):\n",
    "                    temp[0, i] = theta[0, i] * (1 - learning_rate * alpha / train_x.shape[0]) - learning_rate * np.sum(np.matmul((self.cost_function_derivative(standard_x, theta) - center_y).transpose(), standard_x)) / train_x.shape[0]\n",
    "                    if temp[0, i] > alpha / 2:\n",
    "                        temp[0, i] -= alpha / 2\n",
    "                    elif temp[0, i] < alpha / 2:\n",
    "                        temp[0, i] += alpha / 2\n",
    "                    else:\n",
    "                        temp[0, i] = 0\n",
    "                if np.allclose(theta, temp, atol=1e-04) == True:\n",
    "                    break\n",
    "                else:\n",
    "                    theta = temp\n",
    "        for i in range(standard_x.shape[0]):\n",
    "            cost += -1 * center_y[i] * np.log(self.cost_function_derivative(theta, standard_x[i])) - (1 - center_y[i]) * np.log(1 - self.cost_function_derivative(theta, standard_x[i]))\n",
    "        cost = cost / train_x.shape[0]\n",
    "        return theta, cost\n",
    "    def cross_validation(self):\n",
    "        \"\"\"\n",
    "            Find the tuning hyperparameter using 10-fold cross validation.\n",
    "            Return the tuning hyperparameter with the minimum error.\n",
    "        \"\"\"\n",
    "        if self.regularization_type == \"LS\":\n",
    "            pass\n",
    "        else:\n",
    "            min_error = 999999\n",
    "            min_alpha = 0\n",
    "            graph_alpha = []\n",
    "            graph_error = []\n",
    "            for alpha in range(1, 1000, 1):\n",
    "                error = 0\n",
    "                for i in range(10):\n",
    "                    self.valid_x = self.data_x.loc[self.data_x.shape[0] * i // 10: self.data_x.shape[0] * (i + 1) // 10]\n",
    "                    self.valid_y = self.data_y.loc[self.data_x.shape[0] * i // 10: self.data_x.shape[0] * (i + 1) // 10]\n",
    "                    self.train_x = self.data_x.loc[[x for x in range(0, self.data_x.shape[0] * i // 10)] + [x for x in range(self.data_x.shape[0] * (i + 1) // 10, data_x.shape[0])]]\n",
    "                    self.train_y = self.data_y.loc[[x for x in range(0, self.data_x.shape[0] * i // 10)] + [x for x in range(self.data_x.shape[0] * (i + 1) // 10, data_x.shape[0])]]\n",
    "                    self.train_y = self.train_y.to_numpy() - self.train_x.to_numpy()[:, 3]\n",
    "                    self.valid_y = self.valid_y.to_numpy() - self.valid_x.to_numpy()[:, 3]\n",
    "                    self.train_x = self.train_x.to_numpy()\n",
    "                    self.valid_x = self.valid_x.to_numpy()\n",
    "                    theta = self.gradient_descent(self.train_x, self.train_y, alpha / 10, 0.3)[0]\n",
    "                    valid_x_standard = (self.valid_x - np.mean(self.train_x)) / np.std(self.train_x)\n",
    "                    y_predicted = np.matmul(valid_x_standard, theta.transpose()) + np.mean(self.train_y)\n",
    "                    cv_error = 0\n",
    "                    for i in range(self.valid_x.shape[0]):\n",
    "                        cv_error += -1 * self.valid_y[i] * np.log(1e-15 + self.cost_function_derivative(theta, self.train_x[i])) - (1 - self.valid_y[i]) * np.log(1 + 1e-15 - self.cost_function_derivative(theta, self.train_x[i]))\n",
    "                    error += cv_error[0] / valid_x.shape[0]\n",
    "                graph_alpha.append(alpha)\n",
    "                graph_error.append(error / 10)\n",
    "                if error / 10 < min_error:\n",
    "                    min_error = error / 10\n",
    "                    min_alpha = alpha \n",
    "            print(min_error, min_alpha)\n",
    "            draw_hyperparameter(graph_alpha, graph_error)\n",
    "            self.alpha = min_alpha\n",
    "            return min_error, min_alpha\n",
    "    def find_min_lr(self):\n",
    "            \"\"\"\n",
    "                Find the fastest learning rate for the software performance.\n",
    "                Return the learning rate and the running time.\n",
    "            \"\"\"\n",
    "            learning_rate = 0.05\n",
    "            min_time = 999999999\n",
    "            min_rate = learning_rate\n",
    "            for i in range(1, 21):\n",
    "                first_timer = time.perf_counter()\n",
    "                gradient = self.gradient_descent(learning_rate * i, self.train_x, self.train_y, self.alpha)\n",
    "                last_timer = time.perf_counter()\n",
    "                if last_timer - first_timer < min_time:\n",
    "                    min_time = last_timer - first_timer\n",
    "                    min_rate = learning_rate * i\n",
    "            self.learning_rate = min_rate\n",
    "            return min_time, min_rate      \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "            Fit the model into the training set.\n",
    "            Return the parameter vector and the error.\n",
    "\n",
    "        \"\"\"\n",
    "        if(self.regularization_type != \"LS\"):\n",
    "            self.cross_validation()\n",
    "            result = self.gradient_descent(self.train_x, self.train_y, self.alpha, self.learning_rate)\n",
    "        else:\n",
    "            result = self.gradient_descent(self.data_x, self.data_y, 0, self.learning_rate)\n",
    "        self.theta, error = result[0], result[1]\n",
    "        return self.theta, error\n",
    "    def predict(self, test_x, test_y):\n",
    "        \"\"\"\n",
    "            Predict the price increase-decrease of unseen data using the parameter vector of the training data\n",
    "            Return the prediction as ups = 1 and downs = 0, the error and the odds.\n",
    "        \"\"\"\n",
    "        test_x_standard = (test_x - np.mean(test_x)) / np.std(test_x)\n",
    "        test_x_standard = test_x_standard.astype('float64')\n",
    "        self.theta = self.theta.astype('float64') \n",
    "        cost = 1 / (1 + np.exp(np.matmul(test_x_standard, self.theta.transpose())))\n",
    "        odds = [(1 - cost, cost)]\n",
    "        decision_boundary = np.matmul(test_x_standard, self.theta.transpose())\n",
    "        if decision_boundary >= 0:\n",
    "            y_predicted = 1\n",
    "        else:\n",
    "            y_predicted = 0\n",
    "        error = -1 * test_y * np.log(1e-15 + y_predicted) - (1 - test_y) * np.log(1 + 1e-15 - y_predicted)\n",
    "        return y_predicted, error, odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_btc = minute_prices(\"BTC\", \"USD\", 1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_btc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_graph(data_btc, \"Minute\", \"BTC\", \"USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Split the dataset as the training and the test set\n",
    "\"\"\"\n",
    "train_x = data_btc.loc[:1799, 'high':'volumeto']\n",
    "train_y = data_btc.loc[:1799, 'close']\n",
    "valid_x = data_btc.loc[1800:, 'high':'volumeto']\n",
    "print(valid_x)\n",
    "valid_y = data_btc.loc[1800:, 'close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Use the ordinary least squares method.\n",
    "\"\"\"\n",
    "error = 0\n",
    "train_x = data_btc.loc[:data_btc.shape[0] - 1, 'high':'volumeto']\n",
    "train_y = data_btc.loc[:data_btc.shape[0] - 1, 'close']\n",
    "ols = LinearRegression(train_x.to_numpy(), train_y.to_numpy(), valid_x.to_numpy(), valid_y.to_numpy(), 0, \"\")\n",
    "beta_ols = ols.least_squares(train_x.to_numpy(), train_y.to_numpy())\n",
    "error = ols.fit_ls(train_x.to_numpy(), train_y.to_numpy())\n",
    "print(beta_ols, error)\n",
    "data_btc_test = minute_prices(\"BTC\", \"USD\", 1)\n",
    "test_x = data_btc_test.loc[:, 'high':'volumeto']\n",
    "test_y = data_btc_test.loc[:, 'close']\n",
    "test_ols = ols.predict(test_x.to_numpy(), test_y.to_numpy(), 0)\n",
    "print(test_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Use the ridge regression method.\n",
    "\"\"\"\n",
    "min_error = 99999999\n",
    "min_k = 0\n",
    "ridge = None\n",
    "graph_k = []\n",
    "graph_error = []\n",
    "for k in range(1, 500):\n",
    "    error = 0\n",
    "    for i in range(10):\n",
    "        valid_x = data_btc.loc[200 * i: 200 * (i + 1), 'high':'volumeto']\n",
    "        valid_y = data_btc.loc[200 * i: 200 * (i + 1), 'close']\n",
    "        train_x = data_btc.loc[[x for x in range(0, 200 * i)] + [x for x in range(200 * (i + 1), 2000)], 'high':'volumeto']\n",
    "        train_y = data_btc.loc[[x for x in range(0, 200 * i)] + [x for x in range(200 * (i + 1), 2000)], 'close']\n",
    "        ridge = LinearRegression(train_x.to_numpy(), train_y.to_numpy(), valid_x.to_numpy(), valid_y.to_numpy(), k, \"ridge_regression\")\n",
    "        beta_ridge = ridge.ridge_regression(train_x.to_numpy(), train_y.to_numpy(), k)\n",
    "        cv_ridge = ridge.cross_validation(valid_x.to_numpy(), valid_y.to_numpy(), k, \"ridge_regression\")\n",
    "        error += cv_ridge\n",
    "    graph_k.append(k)\n",
    "    graph_error.append(error / 10)\n",
    "    if error / 10 < min_error:\n",
    "        min_error = error / 10\n",
    "        min_k = k\n",
    "print(min_error, min_k)\n",
    "draw_hyperparameter(graph_k, graph_error)\n",
    "data_btc_test = minute_prices(\"BTC\", \"USD\", 1)\n",
    "test_x = data_btc_test.loc[:, 'high':'volumeto']\n",
    "test_y = data_btc_test.loc[:, 'close']\n",
    "test_ridge = ridge.predict(test_x.to_numpy(), test_y.to_numpy(), min_k, \"ridge_regression\")\n",
    "print(test_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Use the lasso method.\n",
    "\"\"\"\n",
    "min_error = 99999999\n",
    "min_k = 0\n",
    "graph_k = []\n",
    "graph_error = []\n",
    "for k in range(1, 500):\n",
    "    error = 0\n",
    "    for i in range(10):\n",
    "        valid_x = data_btc.loc[200 * i: 200 * (i + 1), 'high':'volumeto']\n",
    "        valid_y = data_btc.loc[200 * i: 200 * (i + 1), 'close']\n",
    "        train_x = data_btc.loc[[x for x in range(0, 200 * i)] + [x for x in range(200 * (i + 1), 2000)], 'high':'volumeto']\n",
    "        train_y = data_btc.loc[[x for x in range(0, 200 * i)] + [x for x in range(200 * (i + 1), 2000)], 'close']\n",
    "        lasso = LinearRegression(train_x.to_numpy(), train_y.to_numpy(), valid_x.to_numpy(), valid_y.to_numpy(), k, \"lasso\")\n",
    "        beta_lasso = lasso.lasso(train_x.to_numpy(), train_y.to_numpy(), k)\n",
    "        cv_lasso = lasso.cross_validation(valid_x.to_numpy(), valid_y.to_numpy(), k, \"lasso\")\n",
    "        error += cv_lasso\n",
    "    graph_k.append(k)\n",
    "    graph_error.append(error / 10)\n",
    "    if error / 10 < min_error:\n",
    "        min_error = error / 10\n",
    "        min_k = k\n",
    "print(min_error, min_k)\n",
    "draw_hyperparameter(graph_k, graph_error)\n",
    "data_btc_test = minute_prices(\"BTC\", \"USD\", 1)\n",
    "test_x = data_btc_test.loc[:, 'high':'volumeto']\n",
    "test_y = data_btc_test.loc[:, 'close']\n",
    "test_lasso = lasso.predict(test_x.to_numpy(), test_y.to_numpy(), min_k, \"lasso\")\n",
    "print(test_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Use the slightly different version of kNN.\n",
    "\"\"\"\n",
    "max_k = 0\n",
    "max_accuracy = -1\n",
    "graph_k = []\n",
    "graph_accuracy = []\n",
    "for k in range(3, 100):\n",
    "    accuracy = 0\n",
    "    for i in range(10):\n",
    "        valid_x = data_btc.loc[200 * i: 200 * (i + 1), 'high':'volumeto']\n",
    "        valid_y = data_btc.loc[200 * i: 200 * (i + 1), 'close']\n",
    "        train_x = data_btc.loc[[x for x in range(0, 200 * i)] + [x for x in range(200 * (i + 1), 2000)], 'high':'volumeto']\n",
    "        train_y = data_btc.loc[[x for x in range(0, 200 * i)] + [x for x in range(200 * (i + 1), 2000)], 'close']\n",
    "        knn = kNN(train_x.to_numpy(), valid_x.to_numpy(), train_y.to_numpy(), valid_y.to_numpy(), k, 1)\n",
    "        cv_knn = knn.cross_validation()\n",
    "        accuracy += cv_knn[1][2]\n",
    "    graph_k.append(k)\n",
    "    graph_accuracy.append(accuracy / 10)\n",
    "    if accuracy / 10 > max_accuracy:\n",
    "        max_accuracy = cv_knn[1][2]\n",
    "        max_k = k\n",
    "print(max_accuracy, max_k)\n",
    "draw_hyperparameter(graph_k, graph_accuracy, True)\n",
    "data_btc_test = minute_prices(\"BTC\", \"USD\", 1)\n",
    "test_x = data_btc_test.loc[:, 'high':'volumeto']\n",
    "test_y = data_btc_test.loc[:, 'close']\n",
    "test_knn = knn.predict_test(test_x.to_numpy(), max_k)\n",
    "if test_y.to_numpy()[0] - test_x.to_numpy()[0, 2] > 0:\n",
    "    actual = 1\n",
    "else:\n",
    "    actual = 0\n",
    "print(test_knn == actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Use the logistic regression without using any regularization technique.\n",
    "\"\"\"\n",
    "logistic_ols = None\n",
    "error = 0\n",
    "data_x = (data_btc.loc[:, 'high':'volumeto']).to_numpy()\n",
    "data_y = (data_btc.loc[:, 'close']).to_numpy()\n",
    "ones = np.ones((data_x.shape[0], 1))\n",
    "data_x = np.hstack((ones, data_x))\n",
    "logistic_ols = LogisticRegression(data_x, data_y)\n",
    "result = logistic_ols.fit()\n",
    "cv_log = result[1]\n",
    "error += cv_log\n",
    "print(error, result[0])\n",
    "data_btc_test = minute_prices(\"BTC\", \"USD\", 1)\n",
    "test_x = data_btc_test.loc[1, 'high':'volumeto']\n",
    "test_y = data_btc_test.loc[1, 'close']\n",
    "ones_test = np.ones((1, 1))\n",
    "if test_y - test_x[2] > 0:\n",
    "    test_y = 1\n",
    "else:\n",
    "    test_y = 0\n",
    "test_x = test_x.to_numpy().transpose()\n",
    "test_x = np.append(ones_test, test_x).transpose()\n",
    "test_x = test_x.astype('float64') \n",
    "test_ols = logistic_ols.predict(test_x, test_y)\n",
    "print(test_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Use the logistic regression with using L2 regularization.\n",
    "\"\"\"\n",
    "logistic_ols = None\n",
    "error = 0\n",
    "data_x = (data_btc.loc[:, 'high':'volumeto'])\n",
    "data_y = (data_btc.loc[:, 'close'])\n",
    "ones = [1 for i in range(data_x.shape[0])]\n",
    "data_x[\"bias\"] = ones\n",
    "data_x = data_x[[\"bias\", \"high\", \"low\", \"open\", \"volumefrom\", \"volumeto\"]]\n",
    "logistic_rr = LogisticRegression(data_x, data_y, 1, \"RR\")\n",
    "result = logistic_rr.fit()\n",
    "cv_log = result[1]\n",
    "error += cv_log\n",
    "print(error, result[0])\n",
    "data_btc_test = minute_prices(\"BTC\", \"USD\", 1)\n",
    "test_x = data_btc_test.loc[1, 'high':'volumeto']\n",
    "test_y = data_btc_test.loc[1, 'close']\n",
    "ones_test = np.ones((1, 1))\n",
    "if test_y - test_x[2] > 0:\n",
    "    test_y = 1\n",
    "else:\n",
    "    test_y = 0\n",
    "test_x = test_x.to_numpy().transpose()\n",
    "test_x = np.append(ones_test, test_x).transpose()\n",
    "test_x = test_x.astype('float64') \n",
    "test_ols = logistic_rr.predict(test_x, test_y)\n",
    "print(test_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Use the logistic regression with using L1 regularization.\n",
    "\"\"\"\n",
    "logistic_ols = None\n",
    "error = 0\n",
    "data_x = (data_btc.loc[:, 'high':'volumeto'])\n",
    "data_y = (data_btc.loc[:, 'close'])\n",
    "ones = [1 for i in range(data_x.shape[0])]\n",
    "data_x[\"bias\"] = ones\n",
    "data_x = data_x[[\"bias\", \"high\", \"low\", \"open\", \"volumefrom\", \"volumeto\"]]\n",
    "logistic_lr = LogisticRegression(data_x, data_y, 1, \"LR\")\n",
    "result = logistic_lr.fit()\n",
    "cv_log = result[1]\n",
    "error += cv_log\n",
    "print(error, result[0])\n",
    "data_btc_test = minute_prices(\"BTC\", \"USD\", 1)\n",
    "test_x = data_btc_test.loc[1, 'high':'volumeto']\n",
    "test_y = data_btc_test.loc[1, 'close']\n",
    "ones_test = np.ones((1, 1))\n",
    "if test_y - test_x[2] > 0:\n",
    "    test_y = 1\n",
    "else:\n",
    "    test_y = 0\n",
    "test_x = test_x.to_numpy().transpose()\n",
    "test_x = np.append(ones_test, test_x).transpose()\n",
    "test_x = test_x.astype('float64') \n",
    "test_ols = logistic_lr.predict(test_x, test_y)\n",
    "print(test_ols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
